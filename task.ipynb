{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oYczIFbhTWs"
      },
      "source": [
        "### Тестирование способности LLM к разрешению кореференции \"опросом\" модели с помощью zero-shot промптов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y29_UDEBhcgw"
      },
      "source": [
        "#### Вводные данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h51ZVP-FhhyM"
      },
      "source": [
        "Задача тестирования способности LLM к разрешению кореферентности ([если хочется немного разобраться, что это](https://tpc.ispras.ru/wp-content/uploads/2018/11/lecture-8.pdf)) \"опросом\" модели с помощью zero-shot промптов.\n",
        "\n",
        "Примеры таких промптов можно посмотреть в [этой статье](https://aclanthology.org/2024.lrec-main.145.pdf) в приложении G (на странице 10) и в [этой статье](https://arxiv.org/pdf/2305.14489) в таблице 10 (на странице 13)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLn-AnuHhmcF"
      },
      "source": [
        "### Пункт А"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpFu2Q55hqNC"
      },
      "source": [
        "* Возьмите размеченный на референциальные связи корпус **русскоязычных** текстов — мы предлагаем взять [RuCoCo](https://github.com/vdobrovolskii/rucoco), но вы можете взять любой.\n",
        "\n",
        "* Напишите шаблон промпта для QA-тестирования модели: инструкцию вида \"Вот тебе текст <текст>. Чему кореферентно слово *они* в предложении 2? Дай ответ в формате json\" (можно взять готовые из статей, упомянутых выше, можно немного их изменить, можно написать свои). Это может быть также вопрос в формате mutiple-choice.\n",
        "\n",
        "* Придумайте способ вставлять данные из размеченного корпуса в шаблон промпта так, чтобы можно было автоматически сравнить ответы модели с \"золотым стандартом\" (\"правильным ответом\", извлечённым из корпуса).\n",
        "\n",
        "* Напишите скрипт, позволяющий делать это автоматически для разных упоминаний и разных текстов корпуса, по сути — превратите корпус референциальных связей в массив вопросов для LLM, \"задачник с ключами в конце\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS56IAfPhqRo"
      },
      "source": [
        "#### Золотой стандарт\n",
        "\n",
        "Например, в корпусе есть текст *\"Мама(1) мыла раму(2), и тут она(1) упала.\"*. Согласно разметке *она* кореферентно *мама*. Это и есть золотой станарт — идеальный ответ модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kvu44oShxdO"
      },
      "source": [
        "### Пункт Б"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZb_8nZ_h4T4"
      },
      "source": [
        "Ответы LLM может быть проблематично парсить (они засоряются всякими \"Да, я с радостью решу вашу задачу\" и т. п., иногда даже если формат ответа прописан в инструкции).\n",
        "\n",
        "Возьмите несколько промптов с уже вставленными корпусными данными из пункта А (штук 50-100) и посмотрите, как модель отвечает на вопросы.\n",
        "\n",
        "Если ответы модели нуждаются в обработке, попробуйте придумать способ автоматически превращать их в строго форматированные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50-djRFxh5ZE"
      },
      "source": [
        "### Пункт В\n",
        "\n",
        "#### *если останется время и вдохновение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfap9Mz8h-Ot"
      },
      "source": [
        "В [этой статье](https://arxiv.org/pdf/2305.14489) кроме вопросно-ответного тестирования LLM описан ещё подход, где модель просят \"расставить референциальные теги\" в тексте (пример есть на той же странице 13, под заголовком **Document Template**, и в Фигуре 1 на странице 2)\n",
        "\n",
        "Попробуйте сделать скрипт и для формирования таких промптов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSg4sJONigJb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l60EvcoecZnk"
      },
      "source": [
        "### Тестирование способности LLM к разрешению эллипсиса \"опросом\" модели с помощью zero-shot промптов.\n",
        "\n",
        "Тестирование того, насколько LLM понимает эллипсис, задание аналогичное заданию на кореференцию. Эллипсис — пропуск в тексте или речи элемента предложения, который восстанавливается посредством контекста.\n",
        "Задача Ellipsis Resolution довольна сложна для LLM моделей.\n",
        "\n",
        "Ваша задача:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ_9tpUpaY39"
      },
      "source": [
        "## A\n",
        "Сначала возьмите корпус неразмеченных предложений [отсюда](https://github.com/NotBioWaste905/HSE_contexts_hack/blob/main/task_without_labels.csv). Датасет построен следующим образом -- в 1 столбце предложение с эллипсисом, во 2 столбце предложение с восстановленным эллипсисом.\n",
        "\n",
        "**Если задача с неразмеченным корпусом кажется сложной или невыполнимой, можно сразу взять задание C (см. далее) -- с размеченным корпусом.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxTBRpasa5f_"
      },
      "source": [
        "Напишите шаблон промпта для QA-тестирования модели: инструкцию вида \"Вот тебе предложение <текст>. Предложение содержит эллипсис. Восстанови пропущенную информацию. Дай ответ в формате json\". Модель может возвращать как пропущенные составляющие, так и весь текст (без пропущенной информации).\n",
        "\n",
        "Придумайте способ вставлять данные из размеченного корпуса в шаблон промпта так, чтобы можно было автоматически сравнить ответы модели с \"золотым стандартом\" (\"правильным ответом\", извлечённым из корпуса). Подсказка: можно попробовать использовать инструменты langchain библиотеки, или вы можете предложить свой метод.\n",
        "\n",
        "Напишите скрипт, позволяющий делать это автоматически для разных упоминаний и разных текстов корпуса, по сути — превратите корпус эллипсиса в массив вопросов для LLM, \"задачник с ключами в конце\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLbo1_-VbiUn"
      },
      "source": [
        "## Золотой стандарт"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1LKasqfbsJJ"
      },
      "source": [
        "Например, для item'а\n",
        "\n",
        "_Демократы голосуют за законопроект, а республиканцы против._\n",
        "\n",
        "Золотой стандарт --\n",
        "\n",
        "_Демократы голосуют за законопроект, а республиканцы голосуют против законопроекта._\n",
        "\n",
        "Пропущено \"законопроекта\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aQdpGFDcs1S"
      },
      "source": [
        "## B\n",
        "\n",
        "Ответы LLM может быть проблематично парсить (они засоряются всякими \"Да, я с радостью решу вашу задачу\" и т. п., иногда даже если формат ответа прописан в инструкции).\n",
        "\n",
        "Возьмите несколько промптов с уже вставленными корпусными данными из пункта A (несколько штук) и посмотрите, как модель отвечает на вопросы.\n",
        "\n",
        "Если ответы модели нуждаются в обработке, попробуйте придумать способ автоматически превращать их в строго форматированные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAssSuUqdkt0"
      },
      "source": [
        "## C\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWfea-BCd-mr"
      },
      "source": [
        "Возьмите корпус размеченных предложений [отсюда](https://github.com/NotBioWaste905/HSE_contexts_hack/blob/main/task_with_labels.csv). Датасет построен следующим образом -- в 1 столбце предложение с эллипсисом, место эллипсиса выделено нижним подчеркиванием, во 2 столбце предложение с восстановленным эллипсисом на месте подчеркивания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQrOQfvjeNvy"
      },
      "source": [
        "Сделайте то же самое, что для задания A и B (или в случае, если вы решили взять сразу этот датасет, то это на самом деле те же задания, только для размеченного даттатсета)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIER-vS5ecgy"
      },
      "source": [
        "Сравните полученные результаты: как лучше справляется модель? С размеченным текстом или с неразмеченным? Какие сложности вам встретились?\n",
        "\n",
        "**Точность** выполненного задания не так важна, как **выводы**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzcdxCA1etBU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32brZdqPjEOU"
      },
      "source": [
        "# А какие модели брать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nS3nwG8jJLT"
      },
      "source": [
        "Удобные вам, но хорошего качества.\n",
        "Также можно воспользоваться нашим мини-гайдом и ключами к некоторым моделькам для хакатона\n",
        "[здесь](https://github.com/NotBioWaste905/HSE_contexts_hack/blob/main/connecting_to_api.ipynb).\n",
        "\n",
        "Чтобы упростить работу с выводом модели можете попробовать использовать [structured output](https://python.langchain.com/docs/how_to/structured_output/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "audOL65BjGQw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
