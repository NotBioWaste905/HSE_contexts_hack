{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подключение к API LLM\n",
    "\n",
    "Вам наверняка захочется использовать какие-то модели через API, здесь вы найдете сниппеты которые вы можете просто скопировать в свой код и обращаться к моделькам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YandexGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "iam_token = 't1.9euelZqKxpvKzJyTzMuTmJvMms7OjO3rnpWaj5uSm8eal5nHlMrGjMiLx83l8_c9ZlZD-e9zAHsM_t3z930UVEP573MAewz-zef1656VmseMmsjIz8vPzpSPiciZzJ6L7_zF656VmseMmsjIz8vPzpSPiciZzJ6L.gU-hI-l8zlbOU8XmrYXmo9l6iAoq8uKZp2WGM2n_Kd4tlnvWi-cwSToi2kB4ryptdQyoamICZzGtZaFTkyAcDA'\n",
    "folder_id = 'b1ggrmkcatrn3fgllatd'\n",
    "URL = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
    "\n",
    "def run(iam_token, folder_id, user_text):\n",
    "    # Собираем запрос\n",
    "    data = {}\n",
    "    # Указываем тип модели (https://yandex.cloud/ru/docs/foundation-models/concepts/yandexgpt/models)\n",
    "    data[\"modelUri\"] = f\"gpt://{folder_id}/yandexgpt-lite\"\n",
    "    # Настраиваем опции\n",
    "    data[\"completionOptions\"] = {\"temperature\": 0.3, \"maxTokens\": 1000}\n",
    "    # Указываем контекст для модели\n",
    "    data[\"messages\"] = [\n",
    "        {\"role\": \"system\", \"text\": user_text},\n",
    "        {\"role\": \"user\", \"text\": f\"{user_text}\"},\n",
    "    ]\n",
    "\n",
    "    # Отправляем запрос\n",
    "    response = requests.post(\n",
    "        URL,\n",
    "        headers={\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {iam_token}\"\n",
    "        },\n",
    "        json=data,\n",
    "    ).json()\n",
    "\n",
    "    #Распечатываем результат\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': {'alternatives': [{'message': {'role': 'assistant', 'text': 'В предложении «Мама мыла раму, и тут она упала» слово «она» кореферентно слову «мама».'}, 'status': 'ALTERNATIVE_STATUS_FINAL'}], 'usage': {'inputTextTokens': '69', 'completionTokens': '24', 'totalTokens': '93'}, 'modelVersion': '23.10.2024'}}\n"
     ]
    }
   ],
   "source": [
    "run(iam_token, folder_id, 'Чему кореферентно слово \"она\" в предложении \"Мама(1) мыла раму(2), и тут она( ) упала.\"?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI, Anthropic etc.\n",
    "\n",
    "Названия моделей вы можете посмотреть [здесь](https://vsegpt.ru/Docs/Models)\n",
    "\n",
    "Если у вас возникают какие-то проблемы с ключом/обращением к моделям пишите в ТГ: @andrey_bobr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В предложении \"Мама(1) мыла раму(2), и тут она( ) упала.\" слово \"она\" кореферентно слову \"Мама\". Это означает, что \"она\" ссылается на \"Маму\".\n"
     ]
    }
   ],
   "source": [
    "# Using Langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                   api_key=\"sk-or-vv-16f288696526c0df8f64ed2fba16048201564bac72ee4664ea6138642c527e7c\",\n",
    "                   base_url=\"https://api.vsegpt.ru/v1\",\n",
    "                   temperature=0)\n",
    "\n",
    "print(model.invoke('Чему кореферентно слово \"она\" в предложении \"Мама(1) мыла раму(2), и тут она( ) упала.\"?').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: В данном предложении слово \"она\" кореферентно (относится к тому же лицу или предмету) с \"мама\" (1).\n",
      "\n",
      "Предложение можно разобрать следующим образом:\n",
      "\n",
      "1. \"Мама(1) мыла раму(2)\" - действие выполняется мамой.\n",
      "2. \"и тут она(1) упала\" - здесь \"она\" относится к маме (1), то есть мама упала.\n",
      "\n",
      "Таким образом, слово \"она\" в этом предложении кореферентно слову \"мама\".\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAI API directly\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-or-vv-16f288696526c0df8f64ed2fba16048201564bac72ee4664ea6138642c527e7c\",\n",
    "    base_url=\"https://api.vsegpt.ru/v1\",\n",
    ")\n",
    "\n",
    "prompt = 'Чему кореферентно слово \"она\" в предложении \"Мама(1) мыла раму(2), и тут она( ) упала.\"?'\n",
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "response_big = client.chat.completions.create(\n",
    "    model=\"anthropic/claude-3-haiku\", # id модели из списка моделей - можно использовать OpenAI, Anthropic и пр. меняя только этот параметр\n",
    "    messages=messages,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "response = response_big.choices[0].message.content\n",
    "print(\"Response:\",response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
